{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN14N0t9sQdmZ2qVz2piH1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhusti-D/ML-CS-5783/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 1**\n",
        "\n",
        "The derivation is attached in the github site link.\n",
        "\n",
        "The major difference between the update rule for the trained network \n",
        "for binary classification using log loss function is the cost function. \n",
        "\n",
        "The output layer activation is linear activations while the hidden layer activation functions are non - linear like sigmoid, tanh and so on."
      ],
      "metadata": {
        "id": "-9qgetISoq0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2**"
      ],
      "metadata": {
        "id": "IYVapzI7vDnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) The activation function that can be used for this problem is linear activation function. The other activation functions \n",
        "may make output layer as it avoids modelling fitting for this problem."
      ],
      "metadata": {
        "id": "TCEEZjFLvUVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Only one neuron in the output layer as the final output layer prediction is a linear function based prediction."
      ],
      "metadata": {
        "id": "OfwAX66zvUMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "0Gvm8YTNwPda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj8GCZYqKirk",
        "outputId": "00ea4a9f-4f26-437f-90a9-21d5f0dd7376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "learning_rate = [0.001, 0.005, 0.01, 0.05]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading training and test data"
      ],
      "metadata": {
        "id": "5bG3mhX-wSrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.loadtxt(\"/content/drive/MyDrive/CS5783 Assignment/Assignment 2/X_train.csv\")\n",
        "X_test = np.loadtxt(\"/content/drive/MyDrive/CS5783 Assignment/Assignment 2/X_test.csv\")\n",
        "Y_train = np.loadtxt(\"/content/drive/MyDrive/CS5783 Assignment/Assignment 2/Y_train.csv\")\n",
        "Y_test = np.loadtxt(\"/content/drive/MyDrive/CS5783 Assignment/Assignment 2/Y_test.csv\")  \n",
        "Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
        "Y_test = Y_test.reshape(Y_test.shape[0],1)\n"
      ],
      "metadata": {
        "id": "UYhYAoAyw8ki"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function defnitions for activation functions, predictions, accuracy and more\n",
        "\n",
        "def error(pred_y, y):\n",
        "  return ((pred_y-y)**2).sum()/(pred_y.size)\n",
        "\n",
        "def accuracy(pred_y, y_true):\n",
        "  y_cap = np.sum(y_true)/len(y_true)\n",
        "  acc_reg = np.sum((y_true - pred_y)**2)\n",
        "  acc_tot = np.sum((y_true - y_cap)**2)\n",
        "  acc = 1-(acc_reg/acc_tot)\n",
        "  return acc \n",
        "\n",
        "def sigmoid(n):\n",
        "  return 1/(1+ np.exp(-n))\n",
        "\n",
        "def deriv_sigmoid(n):\n",
        "  return sigmoid(n) * (1- sigmoid(n))\n",
        "\n",
        "def relu(n):\n",
        "  return (np.maximum(0,n))\n",
        "\n",
        "def derivative_relu(n):\n",
        "  n[n<=0] = 0\n",
        "  n[n>0] = 1\n",
        "  return n\n",
        "\n",
        "def lin(n):\n",
        "  return (n)\n",
        "\n",
        "def lin_deriv(n):\n",
        "  return 1\n",
        "\n",
        "def tanh(n):\n",
        "  return (np.exp(n) - np.exp(-n))/(np.exp(n) + np.exp(-n))\n",
        "\n",
        "def deriv_tanh(n):\n",
        "  return 1- (tanh(n))**2\n",
        "\n",
        "def activation(n):\n",
        "  if function == 'sigmoid':\n",
        "    return sigmoid(n)\n",
        "  elif function == 'lin':\n",
        "    return lin(n)\n",
        "  elif function == 'relu':\n",
        "    return relu(n)\n",
        "  elif function == 'tanh':\n",
        "    return tanh(n)\n",
        "\n",
        "def activation_derivative(n):\n",
        "  if function == 'sigmoid':\n",
        "    return deriv_sigmoid(n)\n",
        "  elif function == 'lin':\n",
        "    return lin_deriv(n)\n",
        "  elif function == 'relu':\n",
        "    return derivative_relu(n)\n",
        "  elif function == 'tanh':\n",
        "    return deriv_tanh(n)\n",
        "\n",
        "def lossfunction(self, h, y):\n",
        "  return np.sum((h-y)*(h-y), axis = 0).reshape(1,1)/self.nSamples  "
      ],
      "metadata": {
        "id": "3koiKUxebo3y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Back Propagation**"
      ],
      "metadata": {
        "id": "wdh4Ome8ifJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zmq import LINGER\n",
        "from re import L\n",
        "def back_propagation(W_1, W_2, b_1, b_2):\n",
        "\n",
        "  smple = Y_train.size\n",
        "\n",
        "  Z_1 = np.dot(W_1, X_train.T) + b_1\n",
        "  a_1 = activation(Z_1)\n",
        "\n",
        "  Z_2 = np.dot(W_2, a_1) + b_2\n",
        "  a_2 = lin(Z_2)\n",
        "\n",
        "  for i in range(n_iter):\n",
        "    dZ_2 = a_2 - Y_train.T\n",
        "    dW_2 = np.dot(dZ2, a_1.T)\n",
        "    db_2 = np.sum(dZ2, axis=1) * (1/smple)\n",
        "    dW_1 = np.dot(dZ1, X_train)\n",
        "    db_1 = np.sum(dZ2) * (1/smple)\n",
        "    dZ_1 = np.dot(W_2.T, dZ_2)*activation_derivative(Z_1)\n",
        "\n",
        "  # Weights update\n",
        "\n",
        "  W_1 = W_1 - dW1 * learning_rate\n",
        "  W_2 = W_2 - dW2 * learning_rate\n",
        "  b_1 = b_1 - db1 * learning_rate\n",
        "  b_2 = b_2 - db2 * learning_rate"
      ],
      "metadata": {
        "id": "53_g6kzSiddb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "AO02oFl81B85"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
